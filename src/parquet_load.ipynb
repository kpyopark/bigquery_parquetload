{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal \n",
    "\n",
    "I will test BigQuery upload feature with Parquet file. (This parquet file is downloaded from https://github.com/Teradata/kylo/blob/master/samples/sample-data/parquet/userdata1.parquet)\n",
    "\n",
    "BigQuery tries to detect schema information when to load Parquet format. \n",
    "\n",
    "I will show you how to make predefined schema before to load Parquet format. \n",
    "\n",
    "| column#\t\t| column_name\t |\thive_datatype |\n",
    "|-----------|--------------|----------------|\n",
    "|1\t\t|registration_dttm \t|timestamp|\n",
    "|2\t\t|id \t\t\t|int|\n",
    "|3\t\t|first_name \t\t|string|\n",
    "|4\t\t|last_name \t\t|string|\n",
    "|5\t\t|email \t\t\t|string|\n",
    "|6\t\t|gender \t\t\t|string|\n",
    "|7\t\t|ip_address \t\t|string|\n",
    "|8\t\t|cc \t\t\t|string|\n",
    "|9\t\t|country \t\t|string|\n",
    "|10\t\t|birthdate \t\t|string|\n",
    "|11\t\t|salary \t\t\t|double|\n",
    "|12\t\t|title \t\t\t|string|\n",
    "|13\t\t|comments \t\t|string|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip3 install google.cloud\n",
    "#! pip3 install google.cloud.storage\n",
    "#! pip3 install google.cloud.bigquery\n",
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"<set your credential file path>\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File userdata1.parquet uploaded to bigquery_parquet_upload_test_seoul.\n"
     ]
    }
   ],
   "source": [
    "# Load parquest to GCS\n",
    "\n",
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "# Create a bucket if it doesn't exist\n",
    "bucket_name = \"bigquery_parquet_upload_test_seoul\"\n",
    "\n",
    "storage_client = storage.Client()\n",
    "\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "if not bucket.exists():\n",
    "    bucket.create(location=\"asia-northeast3\")\n",
    "\n",
    "# Upload a file to the bucket\n",
    "file_name = \"userdata1.parquet\"\n",
    "source_file_path = \"../resources/\" + file_name\n",
    "\n",
    "blob = bucket.blob(file_name)\n",
    "if not blob.exists():\n",
    "  blob.upload_from_filename(source_file_path)\n",
    "  print(f\"File {file_name} uploaded to {bucket_name}.\")\n",
    "else:\n",
    "  print(f\"File {file_name} already uploaded.\")\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create table from loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 rows.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# TODO(developer): Set table_id to the ID of the table to create.\n",
    "table_id = \"parquet_test.parquet_default\"\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.PARQUET,\n",
    ")\n",
    "uri = \"gs://{bucket_name}/{file_name}\".format(bucket_name=bucket_name, file_name=file_name)\n",
    "\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, table_id, job_config=job_config\n",
    ")  # Make an API request.\n",
    "\n",
    "\n",
    "load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "destination_table = client.get_table(table_id)\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading, the generated table shows the following schema.\n",
    "\n",
    "| Field name | Type | Mode | \n",
    "|------------|------|------|\n",
    "|registration_dttm\t|TIMESTAMP\t|NULLABLE |\n",
    "|id\t|INTEGER\t|NULLABLE\t\t\t\t|\n",
    "|first_name|\tSTRING\t|NULLABLE|\t\t\t\t\n",
    "|last_name|\tSTRING\t|NULLABLE\t\t|\t\t\n",
    "|email\t|STRING\t|NULLABLE\t\t\t\t|\n",
    "|gender\t|STRING\t|NULLABLE\t\t\t\t|\n",
    "|ip_address\t|STRING\t|NULLABLE\t\t|\t\t\n",
    "|cc\t|STRING\t|NULLABLE\t\t\t\t|\n",
    "|country\t|STRING\t|NULLABLE\t|\t\t\t\n",
    "|birthdate\t|STRING\t|NULLABLE|\t\t\t\t\n",
    "|salary\t|FLOAT\t|NULLABLE\t\t\t|\t\n",
    "|title\t|STRING\t|NULLABLE\t\t\t|\t\n",
    "|comments\t|STRING\t|NULLABLE\t\t|\n",
    "\n",
    "I will try to modify the mode of 'id' field to be 'NOT NULL' and the type of 'salary' from 'FLOAT' to 'STRING'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id = \"parquet_test.parquet_modified\"\n",
    "\n",
    "# Method 1. After create table manually and load from parquet file.\n",
    "\n",
    "results = client.query(\"\"\"\n",
    "create table {table_id} \n",
    "(\n",
    "registration_dttm\tTIMESTAMP,\n",
    "id\tINTEGER\tNOT NULL\t\t\t\t,\n",
    "first_name\tSTRING,\n",
    "last_name\tSTRING,\n",
    "email\tSTRING,\n",
    "gender\tSTRING,\n",
    "ip_address\tSTRING,\n",
    "cc\tSTRING,\n",
    "country\tSTRING,\n",
    "birthdate\tSTRING,\n",
    "salary\tSTRING,\n",
    "title\tSTRING,\n",
    "comments\tSTRING\n",
    ")\n",
    "\"\"\".format(table_id=table_id)).result()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. You can upload the content from parquet file into this new table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequest",
     "evalue": "400 Provided Schema does not match Table turnkey-charter-358922:parquet_test.parquet_modified. Field salary has changed type from STRING to FLOAT",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m uri \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgs://\u001b[39m\u001b[39m{bucket_name}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{file_name}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(bucket_name\u001b[39m=\u001b[39mbucket_name, file_name\u001b[39m=\u001b[39mfile_name)\n\u001b[1;32m      6\u001b[0m load_job \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mload_table_from_uri(\n\u001b[1;32m      7\u001b[0m     uri, table_id, job_config\u001b[39m=\u001b[39mjob_config\n\u001b[1;32m      8\u001b[0m )  \n\u001b[0;32m---> 11\u001b[0m load_job\u001b[39m.\u001b[39;49mresult()  \u001b[39m# Waits for the job to complete.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m destination_table \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mget_table(table_id)\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoaded \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m rows.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(destination_table\u001b[39m.\u001b[39mnum_rows))\n",
      "File \u001b[0;32m~/devel/bigquery_parquetload/lib/python3.9/site-packages/google/cloud/bigquery/job/base.py:922\u001b[0m, in \u001b[0;36m_AsyncJob.result\u001b[0;34m(self, retry, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_begin(retry\u001b[39m=\u001b[39mretry, timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    921\u001b[0m kwargs \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m retry \u001b[39mis\u001b[39;00m DEFAULT_RETRY \u001b[39melse\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mretry\u001b[39m\u001b[39m\"\u001b[39m: retry}\n\u001b[0;32m--> 922\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(_AsyncJob, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/devel/bigquery_parquetload/lib/python3.9/site-packages/google/api_core/future/polling.py:261\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocking_poll(timeout\u001b[39m=\u001b[39mtimeout, retry\u001b[39m=\u001b[39mretry, polling\u001b[39m=\u001b[39mpolling)\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[39m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    263\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "\u001b[0;31mBadRequest\u001b[0m: 400 Provided Schema does not match Table turnkey-charter-358922:parquet_test.parquet_modified. Field salary has changed type from STRING to FLOAT"
     ]
    }
   ],
   "source": [
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.PARQUET,\n",
    ")\n",
    "uri = \"gs://{bucket_name}/{file_name}\".format(bucket_name=bucket_name, file_name=file_name)\n",
    "\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, table_id, job_config=job_config\n",
    ")  \n",
    "\n",
    "\n",
    "load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "destination_table = client.get_table(table_id)\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. it shows error!!\n",
    "\n",
    "We need to specify configuration file into LoadJobConfig operation. \n",
    "\n",
    "https://stackoverflow.com/questions/60230068/error-field-x-has-changed-type-from-numeric-to-float-when-inserting-data-to-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequest",
     "evalue": "400 Provided Schema does not match Table turnkey-charter-358922:parquet_test.parquet_modified. Field salary has changed type from STRING to NUMERIC",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 58\u001b[0m\n\u001b[1;32m     51\u001b[0m uri \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgs://\u001b[39m\u001b[39m{bucket_name}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{file_name}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(bucket_name\u001b[39m=\u001b[39mbucket_name, file_name\u001b[39m=\u001b[39mfile_name)\n\u001b[1;32m     53\u001b[0m load_job \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mload_table_from_uri(\n\u001b[1;32m     54\u001b[0m     uri, table_id, job_config\u001b[39m=\u001b[39mjob_config\n\u001b[1;32m     55\u001b[0m )  \n\u001b[0;32m---> 58\u001b[0m load_job\u001b[39m.\u001b[39;49mresult()  \u001b[39m# Waits for the job to complete.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m destination_table \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mget_table(table_id)\n\u001b[1;32m     61\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoaded \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m rows.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(destination_table\u001b[39m.\u001b[39mnum_rows))\n",
      "File \u001b[0;32m~/devel/bigquery_parquetload/lib/python3.9/site-packages/google/cloud/bigquery/job/base.py:922\u001b[0m, in \u001b[0;36m_AsyncJob.result\u001b[0;34m(self, retry, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_begin(retry\u001b[39m=\u001b[39mretry, timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    921\u001b[0m kwargs \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m retry \u001b[39mis\u001b[39;00m DEFAULT_RETRY \u001b[39melse\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mretry\u001b[39m\u001b[39m\"\u001b[39m: retry}\n\u001b[0;32m--> 922\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(_AsyncJob, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/devel/bigquery_parquetload/lib/python3.9/site-packages/google/api_core/future/polling.py:261\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocking_poll(timeout\u001b[39m=\u001b[39mtimeout, retry\u001b[39m=\u001b[39mretry, polling\u001b[39m=\u001b[39mpolling)\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[39m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    263\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "\u001b[0;31mBadRequest\u001b[0m: 400 Provided Schema does not match Table turnkey-charter-358922:parquet_test.parquet_modified. Field salary has changed type from STRING to NUMERIC"
     ]
    }
   ],
   "source": [
    "my_schema = [\n",
    "  {\n",
    "    \"name\": \"registration_dttm\",\n",
    "    \"type\": \"TIMESTAMP\"\n",
    "  },{\n",
    "    \"name\": \"id\",\n",
    "    \"type\": \"INTEGER\",\n",
    "    \"mode\" : \"REQUIRED\"\n",
    "  },{\n",
    "    \"name\": \"first_name\",\n",
    "    \"type\": \"STRING\"\n",
    "  },{\n",
    "    \"name\": \"last_name\",\n",
    "    \"type\": \"STRING\"\n",
    "  },{\n",
    "    \"name\": \"email\",\n",
    "    \"type\": \"STRING\"\n",
    "  },{\n",
    "    \"name\": \"gender\",\n",
    "    \"type\": \"STRING\"\n",
    "  },{\n",
    "    \"name\": \"ip_address\",\n",
    "    \"type\": \"STRING\"\n",
    "  },{\n",
    "    \"name\": \"cc\",\n",
    "    \"type\": \"STRING\"\n",
    "  },{\n",
    "    \"name\": \"country\",\n",
    "    \"type\": \"STRING\"\n",
    "  },{\n",
    "    \"name\": \"birthdate\",\n",
    "    \"type\": \"STRING\"\n",
    "  },{\n",
    "    \"name\": \"salary\",\n",
    "    \"type\": \"NUMERIC\"\n",
    "  },{\n",
    "    \"name\": \"title\",\n",
    "    \"type\": \"STRING\"\n",
    "  },{\n",
    "    \"name\": \"comments\",\n",
    "    \"type\": \"STRING\"\n",
    "  }\n",
    "]\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.PARQUET,\n",
    "    autodetect=False,\n",
    "    #decimal_target_types=[\"STRING\"],\n",
    "    schema=my_schema\n",
    ")\n",
    "uri = \"gs://{bucket_name}/{file_name}\".format(bucket_name=bucket_name, file_name=file_name)\n",
    "\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, table_id, job_config=job_config\n",
    ")  \n",
    "\n",
    "\n",
    "load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "destination_table = client.get_table(table_id)\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. Numerical Column can't be converted into String directly. \n",
    "\n",
    "OK. The 'type' of 'salary' would be 'NUMERIC' type instead of 'DOUBLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequest",
     "evalue": "400 Error while reading data, error message: Reading column 'salary' of type DOUBLE as BIGNUMERIC type but its logical type is NONE, expecting DECIMAL. File: gs://bigquery_parquet_upload_test_seoul/userdata1.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 60\u001b[0m\n\u001b[1;32m     53\u001b[0m uri \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgs://\u001b[39m\u001b[39m{bucket_name}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{file_name}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(bucket_name\u001b[39m=\u001b[39mbucket_name, file_name\u001b[39m=\u001b[39mfile_name)\n\u001b[1;32m     55\u001b[0m load_job \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mload_table_from_uri(\n\u001b[1;32m     56\u001b[0m     uri, table_id, job_config\u001b[39m=\u001b[39mjob_config\n\u001b[1;32m     57\u001b[0m )  \n\u001b[0;32m---> 60\u001b[0m load_job\u001b[39m.\u001b[39;49mresult()  \u001b[39m# Waits for the job to complete.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m destination_table \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mget_table(table_id)\n\u001b[1;32m     63\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoaded \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m rows.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(destination_table\u001b[39m.\u001b[39mnum_rows))\n",
      "File \u001b[0;32m~/devel/bigquery_parquetload/lib/python3.9/site-packages/google/cloud/bigquery/job/base.py:922\u001b[0m, in \u001b[0;36m_AsyncJob.result\u001b[0;34m(self, retry, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_begin(retry\u001b[39m=\u001b[39mretry, timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    921\u001b[0m kwargs \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m retry \u001b[39mis\u001b[39;00m DEFAULT_RETRY \u001b[39melse\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mretry\u001b[39m\u001b[39m\"\u001b[39m: retry}\n\u001b[0;32m--> 922\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(_AsyncJob, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/devel/bigquery_parquetload/lib/python3.9/site-packages/google/api_core/future/polling.py:261\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocking_poll(timeout\u001b[39m=\u001b[39mtimeout, retry\u001b[39m=\u001b[39mretry, polling\u001b[39m=\u001b[39mpolling)\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[39m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    263\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "\u001b[0;31mBadRequest\u001b[0m: 400 Error while reading data, error message: Reading column 'salary' of type DOUBLE as BIGNUMERIC type but its logical type is NONE, expecting DECIMAL. File: gs://bigquery_parquet_upload_test_seoul/userdata1.parquet"
     ]
    }
   ],
   "source": [
    "table_id = \"parquet_test.parquet_numeric\"\n",
    "\n",
    "my_schema = [\n",
    "  {\n",
    "    \"name\": \"registration_dttm\",\n",
    "    \"type\": \"TIMESTAMP\"\n",
    "  },{\n",
    "    \"name\": \"id\",\n",
    "    \"type\": \"INTEGER\",\n",
    "    \"mode\" : \"REQUIRED\"\n",
    "  },{\n",
    "    \"name\": \"first_name\",\n",
    "    \"type\": \"STRING\"\n",
    "  },{\n",
    "    \"name\": \"last_name\",\n",
    "    \"type\": \"STRING\"\n",
    "  },{\n",
    "    \"name\": \"email\",\n",
    "    \"type\": \"STRING\"\n",
    "  },{\n",
    "    \"name\": \"gender\",\n",
    "    \"type\": \"STRING\"\n",
    "  },{\n",
    "    \"name\": \"ip_address\",\n",
    "    \"type\": \"STRING\"\n",
    "  },{\n",
    "    \"name\": \"cc\",\n",
    "    \"type\": \"STRING\"\n",
    "  },{\n",
    "    \"name\": \"country\",\n",
    "    \"type\": \"STRING\"\n",
    "  },{\n",
    "    \"name\": \"birthdate\",\n",
    "    \"type\": \"STRING\"\n",
    "  },{\n",
    "    \"name\": \"salary\",\n",
    "    \"type\": \"BIGNUMERIC\"\n",
    "  },{\n",
    "    \"name\": \"title\",\n",
    "    \"type\": \"STRING\"\n",
    "  },{\n",
    "    \"name\": \"comments\",\n",
    "    \"type\": \"STRING\"\n",
    "  }\n",
    "]\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.PARQUET,\n",
    "    autodetect=False,\n",
    "    #decimal_target_types=[\"STRING\"],\n",
    "    schema=my_schema\n",
    ")\n",
    "uri = \"gs://{bucket_name}/{file_name}\".format(bucket_name=bucket_name, file_name=file_name)\n",
    "\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, table_id, job_config=job_config\n",
    ")  \n",
    "\n",
    "\n",
    "load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "destination_table = client.get_table(table_id)\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow.. It can't be. \n",
    "\n",
    "Infered Column couldn't be configured with LoadJobConfig options. \n",
    "In the reference document(https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet#type_conversions), it describe the conversion rule. \n",
    "\n",
    "Only 'mode' filed could be changed from 'NULLABLE' to 'REQUIRED'. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigquery_parquetload",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
